# IntelliCLI 统一模型配置系统

## 概述

IntelliCLI 现已完全集成统一模型配置系统，提供交互式配置向导和智能模型管理功能。系统支持多种AI模型供应商，并能根据任务类型自动选择最合适的模型。

## 核心功能

### 1. 交互式配置向导
- 首次运行自动启动
- 逐步引导用户完成配置
- 支持多种供应商选择
- 实时配置验证

### 2. 多供应商支持
- **Ollama**: 本地/远程 Ollama 服务
- **Google Gemini**: Gemini API 服务
- **OpenAI**: OpenAI API 服务

### 3. 模型能力标签系统
- `general`: 通用对话和文本处理
- `code`: 代码生成和编程任务
- `reasoning`: 复杂推理和分析任务
- `vision`: 图像和视觉相关任务

### 4. 智能模型路由
- 根据任务内容自动选择模型
- 基于模型能力标签的匹配
- 支持用户自定义路由规则

## 使用流程

### 首次使用

1. **启动 IntelliCLI**
   ```bash
   python main.py session
   ```

2. **配置向导会自动启动**
   ```
   ⚠️  未找到有效的模型配置
   🚀 将启动配置向导帮助您设置 IntelliCLI
   ```

3. **按照向导完成配置**
   - 选择模型供应商
   - 配置模型信息
   - 设置模型能力
   - 选择主模型

### 配置管理

#### 查看当前配置
```bash
python main.py config
```

#### 重新配置
```bash
python main.py config-wizard
```

#### 重置配置
```bash
python main.py config-reset
```

#### 查看可用模型
```bash
python main.py models
```

## 配置示例

### 完整配置文件
```yaml
models:
  primary: gemma3_local
  providers:
    - alias: gemma3_local
      provider: ollama
      model_name: "gemma3:27b"
      base_url: "http://localhost:11434"
      capabilities: ["general", "code", "reasoning"]
    
    - alias: llava_vision
      provider: ollama
      model_name: "llava:34b"
      base_url: "http://localhost:11434"
      capabilities: ["vision", "general"]
    
    - alias: gemini_pro
      provider: gemini
      model_name: "gemini-1.5-pro-latest"
      capabilities: ["general", "code", "reasoning", "vision"]

tools:
  file_system:
    enabled: true
  shell:
    enabled: true
  system_operations:
    enabled: true
  python_analyzer:
    enabled: true

logging:
  level: INFO
```

### 不同供应商配置

#### Ollama 配置
```yaml
- alias: my_ollama_model
  provider: ollama
  model_name: "gemma3:27b"
  base_url: "http://localhost:11434"
  capabilities: ["general", "code", "reasoning"]
```

#### Gemini 配置
```yaml
- alias: my_gemini_model
  provider: gemini
  model_name: "gemini-1.5-pro-latest"
  capabilities: ["general", "code", "reasoning", "vision"]
```

#### OpenAI 配置
```yaml
- alias: my_openai_model
  provider: openai
  model_name: "gpt-4"
  capabilities: ["general", "code", "reasoning"]
```

## 智能路由示例

### 任务类型自动识别

```bash
# 图像任务 - 自动选择视觉模型
"分析这张截图中的内容"
→ 选择 llava_vision

# 代码任务 - 自动选择代码模型
"写一个Python函数计算斐波那契数列"
→ 选择 gemini_pro

# 推理任务 - 自动选择推理模型
"分析这个算法的时间复杂度"
→ 选择 gemma3_local

# 一般任务 - 选择通用模型
"今天天气怎么样？"
→ 选择 主模型
```

### 路由规则

系统使用以下关键词匹配规则：

1. **视觉任务**
   - 关键词: 图像、图片、照片、截图、识别图、看图
   - 选择: 具有 `vision` 能力的模型

2. **代码任务**
   - 关键词: 代码、编程、函数、class、def、写程序
   - 选择: 具有 `code` 能力的模型

3. **推理任务**
   - 关键词: 分析、推理、解决问题、策略、规划
   - 选择: 具有 `reasoning` 能力的模型

4. **一般任务**
   - 其他所有任务
   - 选择: 具有 `general` 能力的模型

## 环境配置

### API 密钥设置

```bash
# Gemini API 密钥
export GEMINI_API_KEY="your_gemini_api_key"

# OpenAI API 密钥
export OPENAI_API_KEY="your_openai_api_key"
```

### Ollama 服务器

```bash
# 本地 Ollama 服务器
# 默认地址: http://localhost:11434

# 远程 Ollama 服务器
# 配置中设置 base_url: "http://your-server:11434"
```

## 故障排除

### 常见问题

1. **配置文件格式错误**
   - 检查 YAML 语法
   - 确保缩进正确
   - 验证必需字段

2. **模型连接失败**
   - 检查网络连接
   - 验证 API 密钥
   - 确认服务器地址

3. **路由不准确**
   - 检查模型能力配置
   - 更新路由规则
   - 使用更具体的任务描述

### 调试命令

```bash
# 验证配置
python main.py config

# 测试模型连接
python main.py models

# 重置配置
python main.py config-reset
```

## 技术架构

### 核心组件

1. **ModelConfigManager**: 配置管理器
2. **ModelRouter**: 智能路由器
3. **配置验证**: 实时验证系统
4. **用户界面**: 交互式向导

### 文件结构

```
IntelliCLI/
├── config/
│   ├── __init__.py
│   └── model_config.py      # 配置管理器
├── agent/
│   └── model_router.py      # 智能路由器
├── config.yaml              # 配置文件
├── config.yaml.template     # 配置模板
└── main.py                  # 主程序
```

## 扩展开发

### 添加新供应商

1. 在 `ModelConfigManager.providers_info` 中添加供应商信息
2. 在 `models/` 目录创建新的客户端类
3. 更新 `main.py` 中的模型初始化逻辑

### 自定义路由规则

1. 修改 `ModelRouter._build_routing_rules()` 方法
2. 添加新的模式匹配规则
3. 定义模型选择逻辑

### 扩展能力标签

1. 在 `ModelConfigManager.capability_descriptions` 中添加新能力
2. 更新路由规则以支持新能力
3. 在配置向导中添加新选项

## 结语

IntelliCLI 的统一模型配置系统为用户提供了一个简单而强大的AI模型管理解决方案。通过交互式配置向导和智能路由系统，用户可以轻松管理多个AI模型，并根据任务类型自动选择最合适的模型，从而获得最佳的AI辅助体验。 