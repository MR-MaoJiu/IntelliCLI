"""
å†…å®¹æ•´åˆå·¥å…·æ¨¡å—
æä¾›åŸºäºLLMçš„å†…å®¹æ•´åˆã€å¤„ç†å’Œåˆ†æåŠŸèƒ½
"""

import os
import json
import yaml
import re
from typing import Dict, List, Any, Optional, Union
from pathlib import Path

# å…¨å±€æ¨¡å‹å®¢æˆ·ç«¯ï¼Œå°†ç”±ç³»ç»Ÿåˆå§‹åŒ–æ—¶è®¾ç½®
_model_client = None

# Token é™åˆ¶é…ç½®
MAX_INPUT_TOKENS = 100000  # ä¿å®ˆä¼°è®¡ï¼Œç•™å‡ºå®‰å…¨è¾¹è·
OVERLAP_TOKENS = 2000      # å—ä¹‹é—´çš„é‡å éƒ¨åˆ†

def set_model_client(model_client):
    """è®¾ç½®å…¨å±€æ¨¡å‹å®¢æˆ·ç«¯"""
    global _model_client
    _model_client = model_client

def _estimate_tokens(text: str) -> int:
    """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡ï¼ˆç²—ç•¥ä¼°è®¡ï¼‰"""
    # ç²—ç•¥ä¼°è®¡ï¼šä¸­æ–‡ 1 å­—ç¬¦ â‰ˆ 2 tokensï¼Œè‹±æ–‡ 1 å­—ç¬¦ â‰ˆ 0.3 tokens
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    other_chars = len(text) - chinese_chars - english_chars
    
    estimated_tokens = chinese_chars * 2 + english_chars * 0.3 + other_chars * 0.5
    return int(estimated_tokens)

def _split_text_into_chunks(text: str, max_tokens: int = MAX_INPUT_TOKENS, overlap: int = OVERLAP_TOKENS) -> List[str]:
    """å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªchunksï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§"""
    if _estimate_tokens(text) <= max_tokens:
        return [text]
    
    chunks = []
    
    # æŒ‰æ®µè½åˆ†å‰²
    paragraphs = re.split(r'\n\s*\n', text)
    
    current_chunk = ""
    current_tokens = 0
    
    for paragraph in paragraphs:
        paragraph_tokens = _estimate_tokens(paragraph)
        
        # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡é™åˆ¶ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
        if paragraph_tokens > max_tokens:
            # å¦‚æœå½“å‰chunkä¸ä¸ºç©ºï¼Œå…ˆä¿å­˜
            if current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = ""
                current_tokens = 0
            
            # åˆ†å‰²é•¿æ®µè½
            sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s*', paragraph)
            temp_chunk = ""
            temp_tokens = 0
            
            for sentence in sentences:
                if not sentence.strip():
                    continue
                    
                sentence_tokens = _estimate_tokens(sentence)
                
                if temp_tokens + sentence_tokens > max_tokens:
                    if temp_chunk:
                        chunks.append(temp_chunk.strip())
                        # ä¿ç•™é‡å éƒ¨åˆ†
                        overlap_text = temp_chunk[-overlap:] if len(temp_chunk) > overlap else temp_chunk
                        temp_chunk = overlap_text + sentence
                        temp_tokens = _estimate_tokens(temp_chunk)
                    else:
                        # å¦‚æœå•ä¸ªå¥å­å¤ªé•¿ï¼Œå¼ºåˆ¶åˆ†å‰²
                        while sentence:
                            chunk_size = min(max_tokens * 2, len(sentence))  # ç²—ç•¥ä¼°è®¡å­—ç¬¦æ•°
                            chunk_part = sentence[:chunk_size]
                            chunks.append(chunk_part)
                            sentence = sentence[chunk_size - overlap:]
                        temp_chunk = ""
                        temp_tokens = 0
                else:
                    temp_chunk += sentence + "ã€‚"
                    temp_tokens += sentence_tokens
            
            if temp_chunk:
                current_chunk = temp_chunk
                current_tokens = temp_tokens
        else:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼€å§‹æ–°çš„chunk
            if current_tokens + paragraph_tokens > max_tokens:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    # ä¿ç•™é‡å éƒ¨åˆ†
                    overlap_text = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk
                    current_chunk = overlap_text + "\n\n" + paragraph
                    current_tokens = _estimate_tokens(current_chunk)
                else:
                    current_chunk = paragraph
                    current_tokens = paragraph_tokens
            else:
                current_chunk += "\n\n" + paragraph
                current_tokens += paragraph_tokens
    
    # æ·»åŠ æœ€åä¸€ä¸ªchunk
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks

def _merge_extraction_results(results: List[str], output_structure: str = "json") -> str:
    """åˆå¹¶å¤šä¸ªæå–ç»“æœ"""
    if not results:
        return ""
    
    if len(results) == 1:
        return results[0]
    
    if output_structure.lower() == "json":
        # å°è¯•åˆå¹¶JSONç»“æœ
        merged_data = {}
        for result in results:
            try:
                data = json.loads(result)
                if isinstance(data, dict):
                    for key, value in data.items():
                        if key in merged_data:
                            if isinstance(merged_data[key], list) and isinstance(value, list):
                                merged_data[key].extend(value)
                            elif isinstance(merged_data[key], str) and isinstance(value, str):
                                merged_data[key] += f"\n{value}"
                            else:
                                merged_data[key] = value
                        else:
                            merged_data[key] = value
                elif isinstance(data, list):
                    if "items" not in merged_data:
                        merged_data["items"] = []
                    merged_data["items"].extend(data)
            except:
                # å¦‚æœè§£æå¤±è´¥ï¼Œä½œä¸ºæ–‡æœ¬å¤„ç†
                if "raw_text" not in merged_data:
                    merged_data["raw_text"] = []
                merged_data["raw_text"].append(result)
        
        return json.dumps(merged_data, ensure_ascii=False, indent=2)
    else:
        # æ–‡æœ¬æ ¼å¼ç›´æ¥åˆå¹¶
        return "\n\n--- åˆ†å—ç»“æœåˆ†éš”ç¬¦ ---\n\n".join(results)

def integrate_content(
    content: Union[str, List[str], Dict[str, Any]],
    requirement: str,
    output_format: str = "text",
    model_preference: str = "auto",
    max_tokens: int = 2000,
    temperature: float = 0.7
) -> str:
    """
    ä½¿ç”¨LLMå¯¹å†…å®¹è¿›è¡Œæ•´åˆå¤„ç†ï¼Œæ”¯æŒé•¿æ–‡æœ¬è‡ªåŠ¨åˆ†å—å¤„ç†
    
    Args:
        content: è¦æ•´åˆçš„å†…å®¹ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²ã€å­—ç¬¦ä¸²åˆ—è¡¨æˆ–å­—å…¸
        requirement: æ•´åˆè¦æ±‚å’ŒæŒ‡ä»¤
        output_format: è¾“å‡ºæ ¼å¼ ("text", "json", "yaml", "markdown", "html")
        model_preference: æ¨¡å‹åå¥½ ("auto", "code", "reasoning", "general")
        max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        temperature: ç”Ÿæˆæ¸©åº¦
    
    Returns:
        æ•´åˆåçš„å†…å®¹å­—ç¬¦ä¸²
    """
    try:
        # å‡†å¤‡è¾“å…¥å†…å®¹
        formatted_content = _format_input_content(content)
        
        # æ£€æŸ¥å†…å®¹é•¿åº¦ï¼Œå¦‚æœè¶…è¿‡é™åˆ¶åˆ™åˆ†å—å¤„ç†
        content_tokens = _estimate_tokens(formatted_content)
        
        if content_tokens > MAX_INPUT_TOKENS:
            print(f"âš ï¸  å†…å®¹è¿‡é•¿ ({content_tokens} tokens)ï¼Œå¯ç”¨åˆ†å—å¤„ç†...")
            
            # åˆ†å‰²å†…å®¹
            chunks = _split_text_into_chunks(formatted_content)
            print(f"ğŸ“¦ å·²åˆ†å‰²ä¸º {len(chunks)} ä¸ªå¤„ç†å—")
            
            # åˆ†åˆ«å¤„ç†æ¯ä¸ªå—
            chunk_results = []
            for i, chunk in enumerate(chunks):
                print(f"ğŸ”„ å¤„ç†å— {i+1}/{len(chunks)}...")
                
                # ä¸ºæ¯ä¸ªå—æ„å»ºæç¤º
                chunk_prompt = _build_integration_prompt(
                    chunk, 
                    requirement + f"\n\næ³¨æ„ï¼šè¿™æ˜¯ç¬¬{i+1}ä¸ªå¤„ç†å—ï¼Œå…±{len(chunks)}ä¸ªå—ã€‚", 
                    output_format
                )
                
                # è°ƒç”¨LLMå¤„ç†å—
                if _model_client:
                    chunk_result = _model_client.generate(chunk_prompt)
                    chunk_results.append(chunk_result)
                else:
                    return "å†…å®¹æ•´åˆå¤±è´¥: æœªè®¾ç½®æ¨¡å‹å®¢æˆ·ç«¯"
            
            # åˆå¹¶ç»“æœ
            print("ğŸ”— æ­£åœ¨åˆå¹¶å¤„ç†ç»“æœ...")
            final_result = _merge_extraction_results(chunk_results, output_format)
            return final_result
        else:
            # å†…å®¹é•¿åº¦åˆé€‚ï¼Œç›´æ¥å¤„ç†
            integration_prompt = _build_integration_prompt(
                formatted_content, 
                requirement, 
                output_format
            )
            
            # è°ƒç”¨LLMè¿›è¡Œæ•´åˆ
            if _model_client:
                result = _model_client.generate(integration_prompt)
                return result
            else:
                return "å†…å®¹æ•´åˆå¤±è´¥: æœªè®¾ç½®æ¨¡å‹å®¢æˆ·ç«¯"
        
    except Exception as e:
        return f"å†…å®¹æ•´åˆå¤±è´¥: {str(e)}"

def integrate_files(
    file_paths: List[str],
    requirement: str,
    output_format: str = "text",
    include_metadata: bool = True
) -> str:
    """
    æ•´åˆå¤šä¸ªæ–‡ä»¶çš„å†…å®¹
    
    Args:
        file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        requirement: æ•´åˆè¦æ±‚
        output_format: è¾“å‡ºæ ¼å¼
        include_metadata: æ˜¯å¦åŒ…å«æ–‡ä»¶å…ƒæ•°æ®
    
    Returns:
        æ•´åˆåçš„å†…å®¹å­—ç¬¦ä¸²
    """
    try:
        # è¯»å–æ‰€æœ‰æ–‡ä»¶å†…å®¹
        files_content = []
        for file_path in file_paths:
            file_info = _read_file_content(file_path, include_metadata)
            if file_info:
                files_content.append(file_info)
        
        if not files_content:
            return "é”™è¯¯: æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ–‡ä»¶"
        
        # ä½¿ç”¨integrate_contentå¤„ç†
        return integrate_content(
            files_content,
            requirement,
            output_format
        )
        
    except Exception as e:
        return f"æ–‡ä»¶æ•´åˆå¤±è´¥: {str(e)}"

def summarize_content(
    content: Union[str, List[str]],
    summary_type: str = "brief",
    key_points: int = 5,
    language: str = "zh"
) -> str:
    """
    å¯¹å†…å®¹è¿›è¡Œæ‘˜è¦å¤„ç†
    
    Args:
        content: è¦æ‘˜è¦çš„å†…å®¹
        summary_type: æ‘˜è¦ç±»å‹ ("brief", "detailed", "bullet_points", "key_insights")
        key_points: å…³é”®ç‚¹æ•°é‡
        language: è¾“å‡ºè¯­è¨€
    
    Returns:
        æ‘˜è¦ç»“æœå­—ç¬¦ä¸²
    """
    summary_requirements = {
        "brief": f"è¯·å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œç®€æ´æ‘˜è¦ï¼Œç”¨{language}è¾“å‡ºï¼Œæ§åˆ¶åœ¨200å­—ä»¥å†…",
        "detailed": f"è¯·å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œè¯¦ç»†æ‘˜è¦ï¼Œç”¨{language}è¾“å‡ºï¼ŒåŒ…å«ä¸»è¦è§‚ç‚¹å’Œç»†èŠ‚",
        "bullet_points": f"è¯·å°†ä»¥ä¸‹å†…å®¹æ€»ç»“ä¸º{key_points}ä¸ªå…³é”®è¦ç‚¹ï¼Œç”¨{language}è¾“å‡ºï¼Œä½¿ç”¨é¡¹ç›®ç¬¦å·æ ¼å¼",
        "key_insights": f"è¯·æå–ä»¥ä¸‹å†…å®¹çš„{key_points}ä¸ªæ ¸å¿ƒæ´å¯Ÿï¼Œç”¨{language}è¾“å‡ºï¼Œé‡ç‚¹å…³æ³¨é‡è¦å‘ç°å’Œç»“è®º"
    }
    
    requirement = summary_requirements.get(summary_type, summary_requirements["brief"])
    
    return integrate_content(
        content,
        requirement,
        output_format="markdown"
    )

def extract_information(
    content: Union[str, List[str]],
    extraction_targets: List[str],
    output_structure: str = "json"
) -> str:
    """
    ä»å†…å®¹ä¸­æå–ç‰¹å®šä¿¡æ¯ï¼Œæ”¯æŒé•¿æ–‡æœ¬è‡ªåŠ¨åˆ†å—å¤„ç†
    
    Args:
        content: æºå†…å®¹
        extraction_targets: è¦æå–çš„ä¿¡æ¯ç±»å‹åˆ—è¡¨
        output_structure: è¾“å‡ºç»“æ„æ ¼å¼
    
    Returns:
        æå–ç»“æœå­—ç¬¦ä¸²
    """
    targets_str = "ã€".join(extraction_targets)
    requirement = f"""
    è¯·ä»ä»¥ä¸‹å†…å®¹ä¸­æå–ä»¥ä¸‹ä¿¡æ¯ï¼š{targets_str}
    
    è¦æ±‚ï¼š
    1. å‡†ç¡®æå–æ‰€éœ€ä¿¡æ¯
    2. å¦‚æœæŸé¡¹ä¿¡æ¯ä¸å­˜åœ¨ï¼Œæ ‡è®°ä¸º"æœªæ‰¾åˆ°"
    3. ä¿æŒä¿¡æ¯çš„åŸå§‹å‡†ç¡®æ€§
    4. æŒ‰ç…§æŒ‡å®šçš„ç»“æ„æ ¼å¼è¾“å‡º
    5. å¦‚æœè¿™æ˜¯åˆ†å—å¤„ç†çš„ä¸€éƒ¨åˆ†ï¼Œè¯·æå–è¯¥å—ä¸­çš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯
    """
    
    return integrate_content(
        content,
        requirement,
        output_format=output_structure
    )

def transform_format(
    content: str,
    source_format: str,
    target_format: str,
    preserve_structure: bool = True
) -> str:
    """
    è½¬æ¢å†…å®¹æ ¼å¼
    
    Args:
        content: æºå†…å®¹
        source_format: æºæ ¼å¼
        target_format: ç›®æ ‡æ ¼å¼
        preserve_structure: æ˜¯å¦ä¿æŒç»“æ„
    
    Returns:
        è½¬æ¢åçš„å†…å®¹å­—ç¬¦ä¸²
    """
    requirement = f"""
    è¯·å°†ä»¥ä¸‹{source_format}æ ¼å¼çš„å†…å®¹è½¬æ¢ä¸º{target_format}æ ¼å¼ï¼š
    
    è¦æ±‚ï¼š
    1. {'ä¿æŒåŸæœ‰ç»“æ„å’Œå±‚æ¬¡' if preserve_structure else 'å¯ä»¥ä¼˜åŒ–ç»“æ„'}
    2. ç¡®ä¿å†…å®¹å®Œæ•´æ€§
    3. é€‚åº”ç›®æ ‡æ ¼å¼çš„ç‰¹ç‚¹
    4. ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§
    """
    
    return integrate_content(
        content,
        requirement,
        output_format=target_format
    )

def _format_input_content(content: Union[str, List[str], Dict[str, Any]]) -> str:
    """æ ¼å¼åŒ–è¾“å…¥å†…å®¹"""
    if isinstance(content, str):
        return content
    elif isinstance(content, list):
        if all(isinstance(item, str) for item in content):
            return "\n\n".join(f"å†…å®¹ {i+1}:\n{item}" for i, item in enumerate(content))
        else:
            return "\n\n".join(f"é¡¹ç›® {i+1}:\n{json.dumps(item, ensure_ascii=False, indent=2)}" for i, item in enumerate(content))
    elif isinstance(content, dict):
        return json.dumps(content, ensure_ascii=False, indent=2)
    else:
        return str(content)

def _build_integration_prompt(content: str, requirement: str, output_format: str) -> str:
    """æ„å»ºæ•´åˆæç¤º"""
    format_instructions = {
        "text": "ä»¥çº¯æ–‡æœ¬æ ¼å¼è¾“å‡º",
        "json": "ä»¥JSONæ ¼å¼è¾“å‡ºï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®",
        "yaml": "ä»¥YAMLæ ¼å¼è¾“å‡ºï¼Œæ³¨æ„ç¼©è¿›",
        "markdown": "ä»¥Markdownæ ¼å¼è¾“å‡ºï¼Œä½¿ç”¨é€‚å½“çš„æ ‡é¢˜å’Œæ ¼å¼",
        "html": "ä»¥HTMLæ ¼å¼è¾“å‡ºï¼Œä½¿ç”¨è¯­ä¹‰åŒ–æ ‡ç­¾"
    }
    
    format_instruction = format_instructions.get(output_format, "ä»¥æ–‡æœ¬æ ¼å¼è¾“å‡º")
    
    prompt = f"""
ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ•´åˆåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚å¯¹å†…å®¹è¿›è¡Œæ•´åˆå¤„ç†ï¼š

æ•´åˆè¦æ±‚ï¼š
{requirement}

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
{format_instruction}

å¾…æ•´åˆå†…å®¹ï¼š
{content}

è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚è¿›è¡Œæ•´åˆå¤„ç†ï¼Œç¡®ä¿è¾“å‡ºç»“æœå‡†ç¡®ã€å®Œæ•´ã€æ ¼å¼æ­£ç¡®ã€‚
"""
    
    return prompt


def _read_file_content(file_path: str, include_metadata: bool = True) -> Optional[Dict[str, Any]]:
    """è¯»å–æ–‡ä»¶å†…å®¹"""
    try:
        path = Path(file_path)
        if not path.exists():
            return None
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        file_info = {
            "file_path": str(path),
            "content": content
        }
        
        if include_metadata:
            file_info.update({
                "file_name": path.name,
                "file_size": path.stat().st_size,
                "file_extension": path.suffix,
                "modification_time": path.stat().st_mtime
            })
        
        return file_info
        
    except Exception as e:
        return {
            "file_path": file_path,
            "content": f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}",
            "error": True
        }

# ä¾¿æ·å‡½æ•°
def quick_summary(content: str, max_length: int = 200) -> str:
    """å¿«é€Ÿæ‘˜è¦"""
    result = summarize_content(content, "brief")
    if len(result) > max_length:
        return result[:max_length] + "..."
    return result

def quick_extract(content: str, target: str) -> str:
    """å¿«é€Ÿæå–"""
    return extract_information(content, [target])

def quick_transform(content: str, target_format: str) -> str:
    """å¿«é€Ÿæ ¼å¼è½¬æ¢"""
    return transform_format(content, "text", target_format) 