"""
ç½‘ç»œæœç´¢å·¥å…·æ¨¡å—
æ”¯æŒå¤šç§æœç´¢å¼•æ“çš„ç½‘ç»œæœç´¢åŠŸèƒ½ï¼Œå…·å¤‡æ™ºèƒ½åˆ‡æ¢å’Œå›é€€æœºåˆ¶
"""

import os
import yaml
import requests
import re
import time
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta

# æœç´¢å¼•æ“å¥åº·çŠ¶æ€ç®¡ç†
class SearchEngineHealth:
    """æœç´¢å¼•æ“å¥åº·çŠ¶æ€ç®¡ç†å™¨"""
    
    def __init__(self):
        self.health_status = {}
        self.failure_counts = {}
        self.last_success = {}
        self.blacklist_until = {}
    
    def record_success(self, engine: str):
        """è®°å½•æœç´¢æˆåŠŸ"""
        self.health_status[engine] = True
        self.failure_counts[engine] = 0
        self.last_success[engine] = datetime.now()
        # ç§»é™¤é»‘åå•
        if engine in self.blacklist_until:
            del self.blacklist_until[engine]
    
    def record_failure(self, engine: str):
        """è®°å½•æœç´¢å¤±è´¥"""
        self.failure_counts[engine] = self.failure_counts.get(engine, 0) + 1
        self.health_status[engine] = False
        
        # è¿ç»­å¤±è´¥3æ¬¡ååŠ å…¥é»‘åå•5åˆ†é’Ÿ
        if self.failure_counts[engine] >= 3:
            self.blacklist_until[engine] = datetime.now() + timedelta(minutes=5)
    
    def is_available(self, engine: str) -> bool:
        """æ£€æŸ¥å¼•æ“æ˜¯å¦å¯ç”¨"""
        # æ£€æŸ¥æ˜¯å¦åœ¨é»‘åå•ä¸­
        if engine in self.blacklist_until:
            if datetime.now() < self.blacklist_until[engine]:
                return False
            else:
                # é»‘åå•è¿‡æœŸï¼Œç§»é™¤
                del self.blacklist_until[engine]
        
        return True
    
    def get_engine_priority(self, engine: str) -> int:
        """è·å–å¼•æ“ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰"""
        # åŸºç¡€ä¼˜å…ˆçº§
        base_priority = {
            "yahoo": 1,      # å½“å‰æœ€å¯é 
            "google": 2,     # éœ€è¦APIä½†å¾ˆå¯é 
            "bing": 3,       # éœ€è¦API
            "duckduckgo": 4, # å—é™åˆ¶
            "startpage": 5,  # å—é™åˆ¶
            "searx": 6       # ä¸ç¨³å®š
        }.get(engine, 10)
        
        # æ ¹æ®å¤±è´¥æ¬¡æ•°è°ƒæ•´ä¼˜å…ˆçº§
        failure_penalty = self.failure_counts.get(engine, 0) * 2
        
        # æ ¹æ®æœ€åæˆåŠŸæ—¶é—´è°ƒæ•´ä¼˜å…ˆçº§
        time_penalty = 0
        if engine in self.last_success:
            hours_since_success = (datetime.now() - self.last_success[engine]).total_seconds() / 3600
            time_penalty = int(hours_since_success / 24)  # æ¯å¤©å¢åŠ 1ç‚¹ä¼˜å…ˆçº§æƒ©ç½š
        
        return base_priority + failure_penalty + time_penalty

# å…¨å±€å¥åº·çŠ¶æ€ç®¡ç†å™¨
search_health = SearchEngineHealth()

def _load_search_config() -> Dict[str, Any]:
    """åŠ è½½æœç´¢å¼•æ“é…ç½®"""
    config_path = "config.yaml"
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f) or {}
            return config.get("search_engines", {}).get("engines", {})
        except Exception:
            return {}
    return {}

def _get_api_key(engine: str, config_key: str, env_var: str = None) -> Optional[str]:
    """è·å–APIå¯†é’¥ï¼Œä¼˜å…ˆä»é…ç½®æ–‡ä»¶è¯»å–ï¼Œç„¶åä»ç¯å¢ƒå˜é‡"""
    # é¦–å…ˆå°è¯•ä»é…ç½®æ–‡ä»¶è·å–
    config = _load_search_config()
    engine_config = config.get(engine, {})
    if engine_config.get(config_key):
        return engine_config[config_key]
    
    # ç„¶åå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
    if env_var:
        return os.getenv(env_var)
    
    return None

def get_available_engines() -> List[str]:
    """è·å–å½“å‰å¯ç”¨çš„æœç´¢å¼•æ“åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº"""
    config_status = check_search_config()
    available_engines = []
    
    for engine, info in config_status.items():
        if info["available"] and search_health.is_available(engine):
            available_engines.append(engine)
    
    # æŒ‰ä¼˜å…ˆçº§æ’åº
    available_engines.sort(key=lambda x: search_health.get_engine_priority(x))
    
    return available_engines

def web_search(query: str, search_engine: str = "auto", max_results: int = 5, lang: str = "zh-cn") -> Dict[str, Any]:
    """
    æ‰§è¡Œç½‘ç»œæœç´¢å¹¶è¿”å›ç»“æœï¼Œæ”¯æŒæ™ºèƒ½å¼•æ“åˆ‡æ¢
    
    Args:
        query: æœç´¢æŸ¥è¯¢å…³é”®è¯
        search_engine: æœç´¢å¼•æ“ ("auto", "duckduckgo", "google", "bing", "searx", "yahoo", "startpage")
        max_results: æœ€å¤§ç»“æœæ•°é‡
        lang: è¯­è¨€ä»£ç 
    
    Returns:
        åŒ…å«æœç´¢ç»“æœçš„å­—å…¸
    """
    if not query or not query.strip():
        return {"error": "æœç´¢æŸ¥è¯¢ä¸èƒ½ä¸ºç©º"}
    
    query = query.strip()
    max_results = max(1, min(max_results, 20))
    
    # è·å–å¯ç”¨å¼•æ“åˆ—è¡¨
    available_engines = get_available_engines()
    
    if not available_engines:
        return {"error": "å½“å‰æ²¡æœ‰å¯ç”¨çš„æœç´¢å¼•æ“"}
    
    # ç¡®å®šè¦å°è¯•çš„å¼•æ“åˆ—è¡¨
    engines_to_try = []
    
    if search_engine == "auto":
        # è‡ªåŠ¨æ¨¡å¼ï¼šå°è¯•æ‰€æœ‰å¯ç”¨å¼•æ“
        engines_to_try = available_engines
        print(f"ğŸ” è‡ªåŠ¨æœç´¢æ¨¡å¼ï¼Œå°†æŒ‰ä¼˜å…ˆçº§å°è¯•: {', '.join(engines_to_try)}")
    else:
        # æŒ‡å®šå¼•æ“æ¨¡å¼ï¼šé¦–å…ˆå°è¯•æŒ‡å®šå¼•æ“ï¼Œå¤±è´¥åå°è¯•å…¶ä»–å¼•æ“
        if search_engine in available_engines:
            engines_to_try = [search_engine] + [e for e in available_engines if e != search_engine]
        else:
            engines_to_try = available_engines
            print(f"âš ï¸ æŒ‡å®šçš„æœç´¢å¼•æ“ {search_engine} ä¸å¯ç”¨ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å¯ç”¨å¼•æ“")
    
    # å°è¯•æ¯ä¸ªå¼•æ“
    last_error = None
    for i, engine in enumerate(engines_to_try):
        try:
            print(f"ğŸ” å°è¯•æœç´¢å¼•æ“: {engine} ({i+1}/{len(engines_to_try)})")
            
            # æ‰§è¡Œæœç´¢
            result = _execute_search(engine, query, max_results, lang)
            
            if "error" not in result and result.get("total_results", 0) > 0:
                # æœç´¢æˆåŠŸ
                search_health.record_success(engine)
                result["search_info"] = {
                    "engine_used": engine,
                    "attempt_number": i + 1,
                    "total_attempts": len(engines_to_try),
                    "auto_switched": i > 0 or search_engine == "auto"
                }
                print(f"âœ… æœç´¢æˆåŠŸï¼Œä½¿ç”¨å¼•æ“: {engine}")
                return result
            else:
                # æœç´¢å¤±è´¥
                search_health.record_failure(engine)
                last_error = result.get("error", "æœªçŸ¥é”™è¯¯")
                print(f"âŒ {engine} æœç´¢å¤±è´¥: {last_error}")
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªå¼•æ“ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ª
                if i < len(engines_to_try) - 1:
                    print(f"ğŸ”„ åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæœç´¢å¼•æ“...")
                    time.sleep(1)  # çŸ­æš‚å»¶è¿Ÿé¿å…è¿‡å¿«è¯·æ±‚
                    continue
                
        except Exception as e:
            search_health.record_failure(engine)
            last_error = str(e)
            print(f"âŒ {engine} æœç´¢å¼‚å¸¸: {last_error}")
            
            if i < len(engines_to_try) - 1:
                print(f"ğŸ”„ åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæœç´¢å¼•æ“...")
                time.sleep(1)
                continue
    
    # æ‰€æœ‰å¼•æ“éƒ½å¤±è´¥äº†
    return {
        "error": f"æ‰€æœ‰æœç´¢å¼•æ“éƒ½æ— æ³•ä½¿ç”¨ã€‚æœ€åé”™è¯¯: {last_error}",
        "search_info": {
            "engines_tried": engines_to_try,
            "total_attempts": len(engines_to_try),
            "all_failed": True
        }
    }

def _execute_search(engine: str, query: str, max_results: int, lang: str) -> Dict[str, Any]:
    """æ‰§è¡Œå…·ä½“çš„æœç´¢æ“ä½œ"""
    try:
        if engine == "duckduckgo":
            return _search_duckduckgo(query, max_results, lang)
        elif engine == "google":
            return _search_google(query, max_results, lang)
        elif engine == "bing":
            return _search_bing(query, max_results, lang)
        elif engine == "searx":
            return _search_searx(query, max_results, lang)
        elif engine == "yahoo":
            return _search_yahoo(query, max_results, lang)
        elif engine == "startpage":
            return _search_startpage(query, max_results, lang)
        else:
            return {"error": f"ä¸æ”¯æŒçš„æœç´¢å¼•æ“: {engine}"}
    except Exception as e:
        return {"error": f"æœç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"}

def _search_duckduckgo(query: str, max_results: int, lang: str) -> Dict[str, Any]:
    """DuckDuckGoæœç´¢ - ä½¿ç”¨å¤šç§æ–¹æ³•"""
    # é¦–å…ˆå°è¯•å®˜æ–¹API
    try:
        api_result = _search_duckduckgo_api(query, max_results, lang)
        if "error" not in api_result and api_result.get("total_results", 0) > 0:
            return api_result
    except Exception as e:
        print(f"DuckDuckGo APIæœç´¢å¤±è´¥: {e}")
    
    # ç„¶åå°è¯•HTMLæœç´¢
    try:
        html_result = _search_duckduckgo_html(query, max_results, lang)
        if "error" not in html_result and html_result.get("total_results", 0) > 0:
            return html_result
    except Exception as e:
        print(f"DuckDuckGo HTMLæœç´¢å¤±è´¥: {e}")
    
    # æœ€åå°è¯•Liteç‰ˆæœ¬æœç´¢
    try:
        lite_result = _search_duckduckgo_lite(query, max_results, lang)
        if "error" not in lite_result and lite_result.get("total_results", 0) > 0:
            return lite_result
    except Exception as e:
        print(f"DuckDuckGo Liteæœç´¢å¤±è´¥: {e}")
    
    # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†
    return {"error": "DuckDuckGoæœç´¢å¤±è´¥ï¼šæ‰€æœ‰æœç´¢æ–¹æ³•éƒ½æ— æ³•è®¿é—®æˆ–è¿”å›ç»“æœ"}

def _search_duckduckgo_api(query: str, max_results: int, lang: str) -> Dict[str, Any]:
    """ä½¿ç”¨DuckDuckGoçš„Instant Answer API"""
    try:
        search_url = "https://api.duckduckgo.com/"
        params = {
            "q": query,
            "format": "json",
            "no_html": "1",
            "skip_disambig": "1",
            "no_redirect": "1"
        }
        
        response = requests.get(search_url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        results = []
        
        # å¤„ç†å³æ—¶ç­”æ¡ˆ
        if data.get("AbstractText"):
            results.append({
                "title": data.get("Heading", "DuckDuckGoå³æ—¶ç­”æ¡ˆ"),
                "snippet": data.get("AbstractText"),
                "url": data.get("AbstractURL", ""),
                "source": "DuckDuckGoå³æ—¶ç­”æ¡ˆ"
            })
        
        # å¤„ç†ç›¸å…³ä¸»é¢˜
        if data.get("RelatedTopics"):
            for topic in data.get("RelatedTopics", [])[:max_results-len(results)]:
                if isinstance(topic, dict) and topic.get("Text"):
                    results.append({
                        "title": topic.get("FirstURL", "").split("/")[-1].replace("_", " "),
                        "snippet": topic.get("Text"),
                        "url": topic.get("FirstURL", ""),
                        "source": "DuckDuckGoç›¸å…³ä¸»é¢˜"
                    })
        
        return {
            "query": query,
            "search_engine": "DuckDuckGo API",
            "total_results": len(results),
            "results": results[:max_results]
        }
        
    except Exception as e:
        return {"error": f"DuckDuckGo APIæœç´¢å¤±è´¥: {str(e)}"}

def _search_duckduckgo_html(query: str, max_results: int, lang: str) -> Dict[str, Any]:
    """DuckDuckGo HTMLæœç´¢æ–¹æ³• - çœŸå®æœç´¢å®ç°"""
    try:
        from bs4 import BeautifulSoup
        
        # ä½¿ç”¨DuckDuckGoçš„HTMLç‰ˆæœ¬ï¼Œç»•è¿‡JavaScript
        search_url = "https://html.duckduckgo.com/html/"
        
        # æ„å»ºPOSTè¯·æ±‚æ•°æ®
        data = {
            "q": query,
            "b": "",  # èµ·å§‹ç»“æœç¼–å·
            "kl": "cn-zh" if lang.startswith("zh") else "us-en"
        }
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Content-Type": "application/x-www-form-urlencoded",
            "Referer": "https://duckduckgo.com/"
        }
        
        # å‘é€POSTè¯·æ±‚åˆ°HTMLç‰ˆæœ¬
        response = requests.post(search_url, data=data, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        results = []
        
        # DuckDuckGo HTMLç‰ˆæœ¬çš„æœç´¢ç»“æœç»“æ„
        # æŸ¥æ‰¾æœç´¢ç»“æœé“¾æ¥
        result_links = soup.find_all('a', class_='result__a')
        
        for link in result_links:
            try:
                title = link.get_text().strip()
                url = link.get('href')
                
                if not title or not url or len(title) < 3:
                    continue
                
                # æŸ¥æ‰¾å¯¹åº”çš„æè¿°
                snippet = ""
                # å°è¯•æ‰¾åˆ°è¿™ä¸ªé“¾æ¥æ‰€åœ¨çš„ç»“æœå®¹å™¨
                result_container = link.find_parent(['div'], class_=re.compile(r'result'))
                if result_container:
                    snippet_elem = result_container.find(['a'], class_='result__snippet')
                    if snippet_elem:
                        snippet = snippet_elem.get_text().strip()
                
                if not snippet:
                    snippet = f"æ¥è‡ª {url} çš„æœç´¢ç»“æœ"
                
                results.append({
                    "title": title,
                    "snippet": snippet,
                    "url": url,
                    "source": "DuckDuckGo"
                })
                
                if len(results) >= max_results:
                    break
                    
            except Exception as e:
                continue
        
        if not results:
            return {"error": "DuckDuckGo HTMLæœç´¢æœªè¿”å›ä»»ä½•ç»“æœ"}
        
        return {
            "query": query,
            "search_engine": "DuckDuckGo",
            "total_results": len(results),
            "results": results
        }
        
    except ImportError:
        return {"error": "éœ€è¦å®‰è£…beautifulsoup4åº“: pip install beautifulsoup4"}
    except requests.exceptions.RequestException as e:
        return {"error": f"DuckDuckGoç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"}
    except Exception as e:
        return {"error": f"DuckDuckGo HTMLæœç´¢å¤±è´¥: {str(e)}"}

def _search_duckduckgo_lite(query: str, max_results: int, lang: str) -> Dict[str, Any]:
    """DuckDuckGo Liteæœç´¢ - å¤‡ç”¨æ–¹æ³•"""
    try:
        from bs4 import BeautifulSoup
        
        # ç›´æ¥ä½¿ç”¨DuckDuckGoçš„æœç´¢APIç«¯ç‚¹
        search_url = "https://duckduckgo.com/html/"
        
        # ä½¿ç”¨GETè¯·æ±‚
        params = {
            "q": query,
            "kl": "cn-zh" if lang.startswith("zh") else "us-en",
            "s": "0",  # èµ·å§‹ä½ç½®
            "dc": str(max_results)  # ç»“æœæ•°é‡
        }
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Referer": "https://duckduckgo.com/"
        }
        
        response = requests.get(search_url, params=params, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        results = []
        
        # æŸ¥æ‰¾ç»“æœé“¾æ¥ - ä½¿ç”¨æ›´é€šç”¨çš„é€‰æ‹©å™¨
        result_links = soup.find_all('a', href=True)
        
        for link in result_links:
            try:
                href = link.get('href')
                title = link.get_text().strip()
                
                # è¿‡æ»¤æ‰å¯¼èˆªé“¾æ¥å’Œæ— æ•ˆé“¾æ¥
                if (not href or not title or 
                    href.startswith('/') or 
                    href.startswith('#') or
                    'duckduckgo.com' in href or
                    len(title) < 5 or
                    title in ['DuckDuckGo', 'Settings', 'Privacy', 'Terms']):
                    continue
                
                # æŸ¥æ‰¾æè¿°
                snippet = ""
                parent = link.find_parent(['div', 'td', 'tr'])
                if parent:
                    # è·å–çˆ¶å®¹å™¨çš„æ‰€æœ‰æ–‡æœ¬
                    all_text = parent.get_text()
                    text_parts = [part.strip() for part in all_text.split('\n') if part.strip()]
                    
                    # æ‰¾åˆ°æ ‡é¢˜åçš„æè¿°æ–‡æœ¬
                    for i, part in enumerate(text_parts):
                        if title in part and i + 1 < len(text_parts):
                            next_part = text_parts[i + 1]
                            if len(next_part) > 20 and next_part != title:
                                snippet = next_part[:300]
                                break
                
                if not snippet:
                    snippet = f"æ¥è‡ª {href} çš„æœç´¢ç»“æœ"
                
                results.append({
                    "title": title,
                    "snippet": snippet,
                    "url": href,
                    "source": "DuckDuckGo"
                })
                
                if len(results) >= max_results:
                    break
                    
            except Exception as e:
                continue
        
        if not results:
            return {"error": "DuckDuckGo Liteæœç´¢æœªè¿”å›ä»»ä½•æœ‰æ•ˆç»“æœ"}
        
        return {
            "query": query,
            "search_engine": "DuckDuckGo Lite",
            "total_results": len(results),
            "results": results
        }
        
    except ImportError:
        return {"error": "éœ€è¦å®‰è£…beautifulsoup4åº“: pip install beautifulsoup4"}
    except requests.exceptions.RequestException as e:
        return {"error": f"DuckDuckGo Liteç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"}
    except Exception as e:
        return {"error": f"DuckDuckGo Liteæœç´¢å¤±è´¥: {str(e)}"}

def _search_yahoo(query: str, max_results: int, lang: str) -> Dict[str, Any]:
    """Yahooæœç´¢ - çœŸå®æœç´¢å®ç°"""
    try:
        from bs4 import BeautifulSoup
        
        search_url = "https://search.yahoo.com/search"
        params = {
            "p": query,
            "ei": "UTF-8",
            "fl": "0",
            "vl": "lang_" + ("zh-cn" if lang.startswith("zh") else "en")
        }
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Referer": "https://search.yahoo.com/"
        }
        
        response = requests.get(search_url, params=params, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        results = []
        
        # Yahooæœç´¢ç»“æœé€šå¸¸åœ¨ç‰¹å®šçš„CSSé€‰æ‹©å™¨ä¸­
        result_containers = soup.find_all(['div'], class_=re.compile(r'algo|result'))
        
        for container in result_containers:
            try:
                # æŸ¥æ‰¾æ ‡é¢˜é“¾æ¥
                title_link = container.find('a', href=True)
                if not title_link:
                    continue
                
                title = title_link.get_text().strip()
                url = title_link.get('href')
                
                if not title or not url:
                    continue
                
                # æŸ¥æ‰¾æè¿°
                snippet = ""
                snippet_elem = container.find(['p', 'div'], class_=re.compile(r'compText|abstract'))
                if snippet_elem:
                    snippet = snippet_elem.get_text().strip()
                
                if not snippet:
                    snippet = f"æ¥è‡ª {url} çš„Yahooæœç´¢ç»“æœ"
                
                results.append({
                    "title": title,
                    "snippet": snippet,
                    "url": url,
                    "source": "Yahoo"
                })
                
                if len(results) >= max_results:
                    break
                    
            except Exception as e:
                continue
        
        if not results:
            return {"error": "Yahooæœç´¢æœªè¿”å›ä»»ä½•ç»“æœ"}
        
        return {
            "query": query,
            "search_engine": "Yahoo",
            "total_results": len(results),
            "results": results
        }
        
    except ImportError:
        return {"error": "éœ€è¦å®‰è£…beautifulsoup4åº“: pip install beautifulsoup4"}
    except requests.exceptions.RequestException as e:
        return {"error": f"Yahooæœç´¢ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"}
    except Exception as e:
        return {"error": f"Yahooæœç´¢å¤±è´¥: {str(e)}"}

def _search_startpage(query: str, max_results: int, lang: str) -> Dict[str, Any]:
    """Startpageæœç´¢ - çœŸå®æœç´¢å®ç°"""
    try:
        from bs4 import BeautifulSoup
        
        search_url = "https://www.startpage.com/sp/search"
        params = {
            "query": query,
            "language": "chinese" if lang.startswith("zh") else "english",
            "family_filter": "off"
        }
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Referer": "https://www.startpage.com/"
        }
        
        response = requests.get(search_url, params=params, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        results = []
        
        # Startpageæœç´¢ç»“æœç»“æ„
        result_containers = soup.find_all(['div'], class_=re.compile(r'w-gl__result'))
        
        for container in result_containers:
            try:
                # æŸ¥æ‰¾æ ‡é¢˜é“¾æ¥
                title_link = container.find('a', href=True)
                if not title_link:
                    continue
                
                title = title_link.get_text().strip()
                url = title_link.get('href')
                
                if not title or not url:
                    continue
                
                # æŸ¥æ‰¾æè¿°
                snippet = ""
                snippet_elem = container.find(['p', 'div'], class_=re.compile(r'w-gl__description'))
                if snippet_elem:
                    snippet = snippet_elem.get_text().strip()
                
                if not snippet:
                    snippet = f"æ¥è‡ª {url} çš„Startpageæœç´¢ç»“æœ"
                
                results.append({
                    "title": title,
                    "snippet": snippet,
                    "url": url,
                    "source": "Startpage"
                })
                
                if len(results) >= max_results:
                    break
                    
            except Exception as e:
                continue
        
        if not results:
            return {"error": "Startpageæœç´¢æœªè¿”å›ä»»ä½•ç»“æœ"}
        
        return {
            "query": query,
            "search_engine": "Startpage",
            "total_results": len(results),
            "results": results
        }
        
    except ImportError:
        return {"error": "éœ€è¦å®‰è£…beautifulsoup4åº“: pip install beautifulsoup4"}
    except requests.exceptions.RequestException as e:
        return {"error": f"Startpageæœç´¢ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}"}
    except Exception as e:
        return {"error": f"Startpageæœç´¢å¤±è´¥: {str(e)}"}

def _search_google(query: str, max_results: int, lang: str) -> Dict[str, Any]:
    """ä½¿ç”¨Google Custom Search API"""
    api_key = _get_api_key("google", "api_key", "GOOGLE_SEARCH_API_KEY")
    search_engine_id = _get_api_key("google", "search_engine_id", "GOOGLE_SEARCH_ENGINE_ID")
    
    if not api_key or not search_engine_id:
        return {
            "error": "Googleæœç´¢éœ€è¦é…ç½®APIå¯†é’¥å’Œæœç´¢å¼•æ“IDã€‚è¯·åœ¨é…ç½®å‘å¯¼ä¸­è®¾ç½®æˆ–ä½¿ç”¨ç¯å¢ƒå˜é‡: GOOGLE_SEARCH_API_KEY å’Œ GOOGLE_SEARCH_ENGINE_ID"
        }
    
    try:
        search_url = "https://customsearch.googleapis.com/customsearch/v1"
        params = {
            "key": api_key,
            "cx": search_engine_id,
            "q": query,
            "num": min(max_results, 10),
            "lr": f"lang_{lang.split('-')[0]}" if lang else "lang_zh"
        }
        
        response = requests.get(search_url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        results = []
        if "items" in data:
            for item in data["items"]:
                results.append({
                    "title": item.get("title", ""),
                    "snippet": item.get("snippet", ""),
                    "url": item.get("link", ""),
                    "source": "Google"
                })
        
        return {
            "query": query,
            "search_engine": "Google",
            "total_results": len(results),
            "estimated_total": data.get("searchInformation", {}).get("totalResults", "æœªçŸ¥"),
            "results": results
        }
        
    except Exception as e:
        return {"error": f"Googleæœç´¢å¤±è´¥: {str(e)}"}

def _search_bing(query: str, max_results: int, lang: str) -> Dict[str, Any]:
    """ä½¿ç”¨Bing Search API"""
    api_key = _get_api_key("bing", "api_key", "BING_SEARCH_API_KEY")
    
    if not api_key:
        return {"error": "Bingæœç´¢éœ€è¦é…ç½®APIå¯†é’¥ã€‚è¯·åœ¨é…ç½®å‘å¯¼ä¸­è®¾ç½®æˆ–ä½¿ç”¨ç¯å¢ƒå˜é‡: BING_SEARCH_API_KEY"}
    
    try:
        search_url = "https://api.bing.microsoft.com/v7.0/search"
        headers = {"Ocp-Apim-Subscription-Key": api_key}
        params = {
            "q": query,
            "count": min(max_results, 20),
            "mkt": "zh-CN" if lang.startswith("zh") else "en-US"
        }
        
        response = requests.get(search_url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        results = []
        if "webPages" in data and "value" in data["webPages"]:
            for item in data["webPages"]["value"]:
                results.append({
                    "title": item.get("name", ""),
                    "snippet": item.get("snippet", ""),
                    "url": item.get("url", ""),
                    "source": "Bing"
                })
        
        return {
            "query": query,
            "search_engine": "Bing",
            "total_results": len(results),
            "estimated_total": data.get("webPages", {}).get("totalEstimatedMatches", "æœªçŸ¥"),
            "results": results
        }
        
    except Exception as e:
        return {"error": f"Bingæœç´¢å¤±è´¥: {str(e)}"}

def _search_searx(query: str, max_results: int, lang: str) -> Dict[str, Any]:
    """ä½¿ç”¨SearXå¼€æºæœç´¢å¼•æ“"""
    # ä½¿ç”¨å…¬å¼€çš„SearXå®ä¾‹
    searx_instances = [
        "https://searx.be",
        "https://search.sapti.me",
        "https://searx.xyz",
        "https://searx.tiekoetter.com"
    ]
    
    for instance in searx_instances:
        try:
            search_url = f"{instance}/search"
            params = {
                "q": query,
                "format": "json",
                "language": lang,
                "pageno": 1
            }
            
            response = requests.get(search_url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            results = []
            if "results" in data:
                for item in data["results"][:max_results]:
                    results.append({
                        "title": item.get("title", ""),
                        "snippet": item.get("content", ""),
                        "url": item.get("url", ""),
                        "source": f"SearX ({instance})"
                    })
            
            return {
                "query": query,
                "search_engine": f"SearX ({instance})",
                "total_results": len(results),
                "results": results
            }
            
        except Exception:
            continue  # å°è¯•ä¸‹ä¸€ä¸ªå®ä¾‹
    
    return {"error": "æ‰€æœ‰SearXå®ä¾‹éƒ½æ— æ³•è®¿é—®"}

def quick_search(query: str, max_results: int = 3) -> str:
    """
    å¿«é€Ÿæœç´¢ï¼Œè¿”å›æ ¼å¼åŒ–çš„æ–‡æœ¬ç»“æœ
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        max_results: æœ€å¤§ç»“æœæ•°é‡
    
    Returns:
        æ ¼å¼åŒ–çš„æœç´¢ç»“æœæ–‡æœ¬
    """
    result = web_search(query, max_results=max_results)
    
    if "error" in result:
        return f"æœç´¢å‡ºé”™: {result['error']}"
    
    if not result.get("results"):
        return f"æ²¡æœ‰æ‰¾åˆ°å…³äº '{query}' çš„æœç´¢ç»“æœ"
    
    output = [f"ğŸ” æœç´¢ç»“æœ - '{query}' (å…±{result['total_results']}æ¡)"]
    output.append("=" * 50)
    
    for i, item in enumerate(result["results"], 1):
        output.append(f"\n{i}. {item['title']}")
        output.append(f"   {item['snippet']}")
        output.append(f"   ğŸ”— {item['url']}")
        if item.get("source"):
            output.append(f"   ğŸ“° æ¥æº: {item['source']}")
    
    return "\n".join(output)

def search_news(query: str, max_results: int = 5) -> Dict[str, Any]:
    """
    æœç´¢æ–°é—»ä¿¡æ¯
    
    Args:
        query: æ–°é—»æŸ¥è¯¢å…³é”®è¯
        max_results: æœ€å¤§ç»“æœæ•°é‡
    
    Returns:
        æ–°é—»æœç´¢ç»“æœ
    """
    # æ·»åŠ æ–°é—»ç›¸å…³çš„æœç´¢è¯
    news_query = f"{query} æ–°é—» æœ€æ–°"
    return web_search(news_query, max_results=max_results)

def search_academic(query: str, max_results: int = 5) -> Dict[str, Any]:
    """
    æœç´¢å­¦æœ¯ä¿¡æ¯
    
    Args:
        query: å­¦æœ¯æŸ¥è¯¢å…³é”®è¯
        max_results: æœ€å¤§ç»“æœæ•°é‡
    
    Returns:
        å­¦æœ¯æœç´¢ç»“æœ
    """
    # æ·»åŠ å­¦æœ¯ç›¸å…³çš„æœç´¢è¯
    academic_query = f"{query} ç ”ç©¶ è®ºæ–‡ å­¦æœ¯"
    return web_search(academic_query, max_results=max_results)

def get_search_engines() -> List[str]:
    """
    è·å–æ”¯æŒçš„æœç´¢å¼•æ“åˆ—è¡¨
    
    Returns:
        æ”¯æŒçš„æœç´¢å¼•æ“åç§°åˆ—è¡¨
    """
    return ["duckduckgo", "google", "bing", "searx"]

def check_search_config() -> Dict[str, Any]:
    """
    æ£€æŸ¥æœç´¢é…ç½®çŠ¶æ€ï¼Œç»“åˆå¥åº·çŠ¶æ€ç®¡ç†
    
    Returns:
        é…ç½®çŠ¶æ€ä¿¡æ¯
    """
    config_status = {}
    
    # DuckDuckGo - ç”±äºåçˆ¬è™«æœºåˆ¶ï¼Œå®é™…å¯ç”¨æ€§æœ‰é™
    duckduckgo_available = search_health.is_available("duckduckgo")
    config_status["duckduckgo"] = {
        "available": duckduckgo_available,
        "note": "å…è´¹æœç´¢å¼•æ“ï¼Œä½†å—åçˆ¬è™«æœºåˆ¶é™åˆ¶" + ("ï¼Œå½“å‰è¢«æš‚æ—¶ç¦ç”¨" if not duckduckgo_available else ""),
        "source": "å†…ç½®æ”¯æŒï¼ˆå—é™ï¼‰",
        "priority": search_health.get_engine_priority("duckduckgo"),
        "failure_count": search_health.failure_counts.get("duckduckgo", 0)
    }
    
    # Yahoo - å®é™…æµ‹è¯•å¯ç”¨
    yahoo_available = search_health.is_available("yahoo")
    config_status["yahoo"] = {
        "available": yahoo_available,
        "note": "Yahooå…è´¹æœç´¢å¼•æ“ï¼Œå®é™…å¯ç”¨" + ("ï¼Œå½“å‰è¢«æš‚æ—¶ç¦ç”¨" if not yahoo_available else ""),
        "source": "å†…ç½®æ”¯æŒ",
        "priority": search_health.get_engine_priority("yahoo"),
        "failure_count": search_health.failure_counts.get("yahoo", 0)
    }
    
    # Startpage - å—é™åˆ¶
    startpage_available = search_health.is_available("startpage")
    config_status["startpage"] = {
        "available": startpage_available,
        "note": "éšç§ä¿æŠ¤æœç´¢å¼•æ“ï¼Œå—è®¿é—®é™åˆ¶" + ("ï¼Œå½“å‰è¢«æš‚æ—¶ç¦ç”¨" if not startpage_available else ""),
        "source": "å†…ç½®æ”¯æŒï¼ˆå—é™ï¼‰",
        "priority": search_health.get_engine_priority("startpage"),
        "failure_count": search_health.failure_counts.get("startpage", 0)
    }
    
    # Google - éœ€è¦APIå¯†é’¥
    google_api_key = _get_api_key("google", "api_key", "GOOGLE_SEARCH_API_KEY")
    google_engine_id = _get_api_key("google", "search_engine_id", "GOOGLE_SEARCH_ENGINE_ID")
    google_configured = bool(google_api_key and google_engine_id)
    google_available = google_configured and search_health.is_available("google")
    
    config_status["google"] = {
        "available": google_available,
        "note": ("å·²é…ç½®APIå¯†é’¥" if google_configured else "éœ€è¦ API Key å’Œ Search Engine ID") + 
                ("ï¼Œå½“å‰è¢«æš‚æ—¶ç¦ç”¨" if google_configured and not google_available else ""),
        "source": "é…ç½®æ–‡ä»¶" if _load_search_config().get("google") else ("ç¯å¢ƒå˜é‡" if google_configured else "æœªé…ç½®"),
        "priority": search_health.get_engine_priority("google"),
        "failure_count": search_health.failure_counts.get("google", 0)
    }
    
    # Bing - éœ€è¦APIå¯†é’¥
    bing_api_key = _get_api_key("bing", "api_key", "BING_SEARCH_API_KEY")
    bing_configured = bool(bing_api_key)
    bing_available = bing_configured and search_health.is_available("bing")
    
    config_status["bing"] = {
        "available": bing_available,
        "note": ("å·²é…ç½®APIå¯†é’¥" if bing_configured else "éœ€è¦ API Key") +
                ("ï¼Œå½“å‰è¢«æš‚æ—¶ç¦ç”¨" if bing_configured and not bing_available else ""),
        "source": "é…ç½®æ–‡ä»¶" if _load_search_config().get("bing") else ("ç¯å¢ƒå˜é‡" if bing_configured else "æœªé…ç½®"),
        "priority": search_health.get_engine_priority("bing"),
        "failure_count": search_health.failure_counts.get("bing", 0)
    }
    
    # SearX - å…¬å¼€å®ä¾‹é€šå¸¸ä¸ç¨³å®š
    searx_available = search_health.is_available("searx")
    config_status["searx"] = {
        "available": searx_available,
        "note": "ä½¿ç”¨å…¬å¼€å®ä¾‹ï¼Œè¿æ¥ä¸ç¨³å®š" + ("ï¼Œå½“å‰è¢«æš‚æ—¶ç¦ç”¨" if not searx_available else ""),
        "source": "å†…ç½®æ”¯æŒï¼ˆä¸ç¨³å®šï¼‰",
        "priority": search_health.get_engine_priority("searx"),
        "failure_count": search_health.failure_counts.get("searx", 0)
    }
    
    return config_status

def get_search_health_report() -> Dict[str, Any]:
    """è·å–æœç´¢å¼•æ“å¥åº·çŠ¶æ€æŠ¥å‘Š"""
    available_engines = get_available_engines()
    
    report = {
        "available_engines": available_engines,
        "total_available": len(available_engines),
        "engine_priorities": {engine: search_health.get_engine_priority(engine) for engine in available_engines},
        "failure_counts": dict(search_health.failure_counts),
        "blacklisted_engines": [],
        "last_success": {}
    }
    
    # æ£€æŸ¥é»‘åå•å¼•æ“
    current_time = datetime.now()
    for engine, blacklist_time in search_health.blacklist_until.items():
        if current_time < blacklist_time:
            remaining_minutes = int((blacklist_time - current_time).total_seconds() / 60)
            report["blacklisted_engines"].append({
                "engine": engine,
                "remaining_minutes": remaining_minutes
            })
    
    # æœ€åæˆåŠŸæ—¶é—´
    for engine, success_time in search_health.last_success.items():
        hours_ago = int((current_time - success_time).total_seconds() / 3600)
        report["last_success"][engine] = f"{hours_ago}å°æ—¶å‰" if hours_ago > 0 else "åˆšåˆš"
    
    return report 